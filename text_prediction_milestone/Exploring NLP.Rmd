Exploring Text Prediction With Twitter Messages, Blog Posts, and News Articles
========================================================

This report is a concise exploratory analysis to understand a collection of various text files that will be used to build a predictive text algorithm and web application. These text files, used as the training dataset, are a combination of twitter messages, blog posts, and news articles. The primary goal of this report is to create basic summaries of the three files including word counts, line counts, basic data tables, plots, and histograms in order to illstrate features of the data. 

The fundamental components of this analysis can benefit the company in many ways. To list two; embedding text auto-complete in the internal email system or propriety productivity report generator and classifying previous client proposals as either successful or failures in order to create future rate proposals. The data can be downloaded from this [link]("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip") and the entire underlying code for this report can be found [here]("http://github.com/FrankRuns/natlangproc").     

## Data Aquisition

Download and read the English data set. Using terminal commands we found the following line and word counts

* Twitter Messages: 2,360,148 lines, 30,374,206 words
* Blog Posts: 899,288 lines, 37,334,690 words
* News Articles: 1,010,242 lines, 34,372,720 words

Despite having the largest number of lines, the twitter.txt file is actually the smallest in terms of word count and size. File sizes are as follows:

* Twitter Messages: 159 MB
* Blog Posts: 196 MB
* News Articles: 200 MB

The size of these files creates a memory bandwidth challenge and for that reason we will only load a sample of each of the three files. It is assumed that the entries (entries meaning lines) in all files are randomly distributed.  

```{r echo=FALSE}
setwd("/Users/frankCorrigan/natlp/en_US")
data <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
tweetsFile <- "en_US.twitter.txt"; blogsFile <- "en_US.blogs.txt"; newsFile <- "en_US.news.txt"
percentage <- 0.01
tcon <- file(tweetsFile, "r"); bcon <- file(blogsFile, "r"); ncon <- file(newsFile, "r")
tweets <- readLines(tcon, n=(2300000*percentage)); blogs <- readLines(bcon, n=(900000*percentage)); news <- readLines(ncon, n=(100000*percentage))
close(tcon); close(bcon); close(ncon)
```

```{r echo=FALSE}
paste("We will use", length(tweets), "tweets,", length(blogs), "blog posts,", "and", length(news), "news articles")
```

With the data loaded, we seek to look at the characteristics of each text source. We'll want to know about typical entry length, word count frequencies, and any prevelant word associations. We make a quick visual inspection of the number of characters in each text source. 

```{r echo=FALSE, fig.height=4, fig.width=12}
par(mfrow=c(1,3))
hist(nchar(tweets), , xlab="Tweets", main="Tweet Length Frequency")
hist(nchar(blogs), xlab="Blogs", main="Blog Length Frequency")
hist(nchar(news), xlab="News", main="News Length Frequency")
```

It is immediately clear that one is not like the others. Twitter messages are significantly shorter and have a smoother, although still right skewed, distribution. In order to look deeper into word and phrase frequency, we store each text file into a corpus (a collection of written texts).

## Into the Corpus

```{r echo=FALSE}
library(tm)
# text files in list and make corpora
tweets.c <- Corpus(VectorSource(tweets))
blogs.c <- Corpus(VectorSource(blogs))
news.c <- Corpus(VectorSource(news))
# define and apply clean up functions
#skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace)
options(mc.cores=1)
tweets.c <- tm_map(tweets.c, FUN=tm_reduce, tmFuns = funcs)
tweets.c <- tm_map(tweets.c, removeWords, stopwords("english"))
tweets.c <- tm_map(tweets.c, stripWhitespace)
blogs.c <- tm_map(blogs.c, FUN=tm_reduce, tmFuns = funcs)
blogs.c <- tm_map(blogs.c, removeWords, stopwords("english"))
blogs.c <- tm_map(blogs.c, stripWhitespace)
news.c <- tm_map(news.c, FUN=tm_reduce, tmFuns = funcs)
news.c <- tm_map(news.c, removeWords, stopwords("english"))
news.c <- tm_map(news.c, stripWhitespace)
```

After initial clean up of the corpora which includes transforming the text to lower case, removing punctuation and numbers, and removing common stopwords we can view snapshots of each corpus:

```{r echo=FALSE}
paste("Twitter Corpus Snapshot: Summary of Whole Corpus")
summary(tweets.c)
paste("Blogs Corpus Snapshot: Summary of First Two Entries")
inspect(blogs.c[1:2])
paste("News Corpus Snapshot: Summary of Entries # 50-52")
inspect(news.c[50:52])
```

This gives us a sense of our corpora from which we create the term document matrices. 

```{r echo=FALSE}
tweets.m <- TermDocumentMatrix(tweets.c)
blogs.m <- TermDocumentMatrix(blogs.c)
news.m <- TermDocumentMatrix(news.c)
```

The number of words that occur more than 200 times in each text source varies greatly. It is interesting to see similarity between twitter messages and blogs with news standing alone this time.

```{r echo=FALSE}
paste("Twitter message terms that occur more than 200 times:", length(findFreqTerms(tweets.m, 200)))
paste("Blog post terms that occur more than 200 times:", length(findFreqTerms(blogs.m, 200)))
paste("News article terms that occur more than 200 times:", length(findFreqTerms(news.m, 200)))
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=2.5, fig.width=12}
# Word Cloud
library(wordcloud)
par(mfrow=c(1,3))
# Tweets Word Cloud
tweets.dense <- as.matrix(tweets.m)
wordcloud(rownames(tweets.dense), rowSums(tweets.dense), min.freq = 200, color = brewer.pal(6,"Dark2"))
# Blogs Word Cloud
blogs.dense <- as.matrix(blogs.m)
wordcloud(rownames(blogs.dense), rowSums(blogs.dense), min.freq = 200, color = brewer.pal(6, "Dark2"))
# News Word Cloud
news.dense <- as.matrix(news.m)
wordcloud(rownames(news.dense), rowSums(news.dense), min.freq = 200, color = brewer.pal(6, "Dark2"))
```

Finally, words tend to happen together. For the most frequent word found in each file sample, which words are most stringly associated with that word.

```{r echo=FALSE}
# Tweets Most Frequent Term and Associated Words
tweets.1 <- sort(rowSums(tweets.dense), decreasing=TRUE)[1]
findAssocs(tweets.m, names(tweets.1), 0.06)
# Blogs Most Frequent Term and Associated Words
blogs.1 <- sort(rowSums(blogs.dense), decreasing=TRUE)[1]
findAssocs(blogs.m, names(blogs.1), 0.12)
# News Most Frequent Term and Associated Words
news.1 <- sort(rowSums(news.dense), decreasing=TRUE)[1]
findAssocs(news.m, names(news.1), 0.18)
```

## Summary and Next Steps

We are working with three large text files in order to build a predictive text algorithm. The blogs and news files have approxamitely 1mm entries totat and the twitter file has twice that total, of which we used a 1% sample. The blogs and news file entries have similar structures (# charachters, average length of entry), but the blogs and twitter entries tend to share similar languistic characteristics (frequent words). Word associations are stronger in the news file than the blogs and twitter files which may indicate less word variety in news entries than blog entries and twitter messages. As a final check, let's look at the number of unique terms in each term document matrix. As we would expect, the news matrix has fewer terms than the other two.

```{r echo=FALSE}
paste("The twitter term doc matrix has", dim(tweets.m)[1], "unique terms.")
paste("The blogs term doc matrix has", dim(blogs.m)[1], "unique terms.")
paste("The news term doc matrix has", dim(news.m)[1], "unique terms.")
```

Each file adds to the diversity of our 'natural language' upon which to build the Shiny app. In order to build the predictive text algorithm these are the steps that will be taken.

1. Use ngram term document matrix to build simple predictive model. This will utilize 4gram, 3gram, and bigram models.
2. Explore better options for handling swear words. This app will not suggest profanity.
3. Research methods to improve speed of initial model. This includes alternative R packages and hosted servers such as Amazon Web Services.
4. Improve accuracy of the inital predictive model. This includes better understanding of punctuation and word associations. Perhaps classifying the document as a twitter message, blog post, or news article will help in selecting the 'correct' prediction.
5. Contemplate visualizations that will be useful to end user. At this point, a list of likely next words (rather than just one) and the cooresponding probability is the only thing that comes to mind.
6. Build the r.server and r.ui files for the shiny app.

Any and all suggestions for improvement of workflow greatly, greatly, greatly appreciated. Thank you for reading this report.
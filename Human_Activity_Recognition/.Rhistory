rm(list=ls())
?mtcars
cor(mtcars, use="complete.obs", method="kendall")
t.test(mtcars$mpg~mtcars$am, conf.level=0.95)
mean(mtcars$mpg~mtcars$am)
mean(subset(mtcars, mtcars$am == 0))
tapply(mtcars$mpg, mtcars$am, mean)
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
tapply(mtcars$mpg, mtcars$am, mean)
step.model <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2)
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", ylim=c(0,35), main="Frank Chart", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$PredictedMPG, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.05, c("Predicted","Actual"), fill=c("black", "blue"))
axis(1, at=1:32, labels=row.names(mtcars), las=3)
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
FrankDF
FrankDF$Index
names(FrankDF)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2)
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", ylim=c(0,35), main="Frank Chart", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.05, c("Predicted","Actual"), fill=c("black", "blue"))
axis(1, at=1:32, labels=row.names(mtcars), las=3)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, )
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", ylim=c(0,35), main="Frank Chart", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", c("Predicted","Actual"), fill=c("black", "blue"))
axis(1, at=1:32, labels=row.names(mtcars), las=3)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, )
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", ylim=c(0,35), main="Frank Chart", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
axis(1, at=1:32, labels=row.names(mtcars), las=3)
par(las=2, )
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", ylim=c(0,35), main="Predicted vs. Actual MPGs", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
axis(1, at=1:32, labels=row.names(mtcars), las=3)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, )
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", ylim=c(0,35), main="Predicted vs. Actual MPGs", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
par(las=2, )
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab=NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", xlab="Car", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
?plot
par(las=2, )
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
par(las=2, mar=(8, 2, 4, 4))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
par(las=2, mar=c(20, 2, 4, 4))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
par(las=2, mar=c(8, 4, 4, 4))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
data(mtcars)
class(mtcars)
sum(is.na(mtcars))
str(mtcars)
# load the data
data(mtcars)
# what type of data grouping are we working with?
class(mtcars)
# how many missing values are in this dataset?
sum(is.na(mtcars))
# understand the structure of the variables
str(mtcars)
# what is the avg mpg of automatic and manual cars in this dataset?
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
tapply(mtcars$mpg, mtcars$am, mean)
t.test(mtcars$mpg~mtcars$am, conf.level=0.95)
Factors Effecting Fuel Economy
# load the data
data(mtcars)
# what type of data grouping are we working with?
class(mtcars)
# how many missing values are in this dataset?
sum(is.na(mtcars))
# understand the structure of the variables
str(mtcars)
# what is the avg mpg of automatic and manual cars in this dataset?
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
tapply(mtcars$mpg, mtcars$am, mean)
# Is this difference in the mean statistically significant?
t.test(mtcars$mpg~mtcars$am, conf.level=0.95)
# p-value is less than .005 so YES!
# How can we model the data to predict mpg of an automobile in this dataset?
model <- lm(mpg ~ ., data = mtcars)
summary(model)
# Using every variable will lead to overfitting. Plus, correlation between variables leads to unreliable results. Try again.
# Can we use the stepwise selection process to find the best fit model?
step.model <- step(lm(data=mtcars, mpg~.), trace=0, steps=10000)
summary(step.model)
# Independent variables in this model are weight, 1/4-mile time, and transmission. But transmission is a categorical variable... so let's control for that variable.
# This 3rd model uses transmission as a control variable and weight and 1/4-mile time as the independent variables.
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
# Interpretation: Automatic transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 3.18 * 2.32 + 0.83 * 18.61 = 22.04. Manual transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 6.1 * 2.32 + 1.4 * 18.61 = 25.87. This model appears to agree with our prior notion (and the conventional wisdom) that manual cars get better gas mileage than automatic transmission cars.
# Goodness of Fit
# What do the residuals say about the fit of the model?
plot(step.model)
# Now check how the model's predicted values for the dataset's automobiles stack up against the actual values
library(Hmisc)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
# Not bad, but not good. 10 out of 32 value fall outside our 95% confidence error bars. This is where further investigation can be done.
Factors Effecting Fuel Economy
# load the data
data(mtcars)
# what type of data grouping are we working with?
class(mtcars)
# how many missing values are in this dataset?
sum(is.na(mtcars))
# understand the structure of the variables
str(mtcars)
# what is the avg mpg of automatic and manual cars in this dataset?
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
tapply(mtcars$mpg, mtcars$am, mean)
# Is this difference in the mean statistically significant?
t.test(mtcars$mpg~mtcars$am, conf.level=0.95)
# p-value is less than .005 so YES!
# How can we model the data to predict mpg of an automobile in this dataset?
model <- lm(mpg ~ ., data = mtcars)
summary(model)
# Using every variable will lead to overfitting. Plus, correlation between variables leads to unreliable results. Try again.
# Can we use the stepwise selection process to find the best fit model?
step.model <- step(lm(data=mtcars, mpg~.), trace=0, steps=10000)
summary(step.model)
# Independent variables in this model are weight, 1/4-mile time, and transmission. But transmission is a categorical variable... so let's control for that variable.
# This 3rd model uses transmission as a control variable and weight and 1/4-mile time as the independent variables.
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
# Interpretation: Automatic transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 3.18 * 2.32 + 0.83 * 18.61 = 22.04. Manual transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 6.1 * 2.32 + 1.4 * 18.61 = 25.87. This model appears to agree with our prior notion (and the conventional wisdom) that manual cars get better gas mileage than automatic transmission cars.
# Goodness of Fit
# What do the residuals say about the fit of the model?
plot(step.model2)
# Now check how the model's predicted values for the dataset's automobiles stack up against the actual values
library(Hmisc)
p <- predict(step.model2, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
# Not bad, but not good. 10 out of 32 value fall outside our 95% confidence error bars. This is where further investigation can be done.
Factors Effecting Fuel Economy
# load the data
data(mtcars)
# what type of data grouping are we working with?
class(mtcars)
# how many missing values are in this dataset?
sum(is.na(mtcars))
# understand the structure of the variables
str(mtcars)
# what is the avg mpg of automatic and manual cars in this dataset?
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
tapply(mtcars$mpg, mtcars$am, mean)
# Is this difference in the mean statistically significant?
t.test(mtcars$mpg~mtcars$am, conf.level=0.95)
# p-value is less than .005 so YES!
# How can we model the data to predict mpg of an automobile in this dataset?
model <- lm(mpg ~ ., data = mtcars)
summary(model)
# Using every variable will lead to overfitting. Plus, correlation between variables leads to unreliable results. Try again.
# Can we use the stepwise selection process to find the best fit model?
step.model <- step(lm(data=mtcars, mpg~.), trace=0, steps=10000)
summary(step.model)
# Independent variables in this model are weight, 1/4-mile time, and transmission. But transmission is a categorical variable... so let's control for that variable.
# This 3rd model uses transmission as a control variable and weight and 1/4-mile time as the independent variables.
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
# Interpretation: Automatic transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 3.18 * 2.32 + 0.83 * 18.61 = 22.04. Manual transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 6.1 * 2.32 + 1.4 * 18.61 = 25.87. This model appears to agree with our prior notion (and the conventional wisdom) that manual cars get better gas mileage than automatic transmission cars.
# Goodness of Fit
# What do the residuals say about the fit of the model?
plot(step.model2)
p <- predict(step.model2, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
# Not bad, but not good. 10 out of 32 value fall outside our 95% confidence error bars. This is where further investigation can be done.
clear()
cat("\014")
Factors Effecting Fuel Economy
# load the data
data(mtcars)
# what type of data grouping are we working with?
class(mtcars)
# how many missing values are in this dataset?
sum(is.na(mtcars))
# understand the structure of the variables
str(mtcars)
# what is the avg mpg of automatic and manual cars in this dataset?
mtcars$am <- as.factor(mtcars$am)
levels(mtcars$am) <- c("Automatic", "Manual")
tapply(mtcars$mpg, mtcars$am, mean)
# Is this difference in the mean statistically significant?
t.test(mtcars$mpg~mtcars$am, conf.level=0.95)
# p-value is less than .005 so YES!
# How can we model the data to predict mpg of an automobile in this dataset?
model <- lm(mpg ~ ., data = mtcars)
summary(model)
# Using every variable will lead to overfitting. Plus, correlation between variables leads to unreliable results. Try again.
# Can we use the stepwise selection process to find the best fit model?
step.model <- step(lm(data=mtcars, mpg~.), trace=0, steps=10000)
summary(step.model)
# Independent variables in this model are weight, 1/4-mile time, and transmission. But transmission is a categorical variable... so let's control for that variable.
# This 3rd model uses transmission as a control variable and weight and 1/4-mile time as the independent variables.
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
# Interpretation: Automatic transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 3.18 * 2.32 + 0.83 * 18.61 = 22.04. Manual transmission car with wt is 2.32 and qsec is 18.61 MPG = 13.97 - 6.1 * 2.32 + 1.4 * 18.61 = 25.87. This model appears to agree with our prior notion (and the conventional wisdom) that manual cars get better gas mileage than automatic transmission cars.
# Goodness of Fit
# What do the residuals say about the fit of the model?
plot(step.model2)
p <- predict(step.model2, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
# Not bad, but not good. 10 out of 32 value fall outside our 95% confidence error bars. This is where further investigation can be done.
?hegith
?height
?boxplot
plot(mtcars)
This looks and sounds in line with conventional wisdom that manuals get better fuel economy than automatics, but perhaps this is only a statistical anomaly of our dataset. Let's use an [independent t-test](http://www.gla.ac.uk/sums/users/narjis/stroke/indept1.html) to find out. The results of this t-test can be found in the full report. In this test the p-value is 0.001374. Translation for data people: assuming the null hypothesis is true (which is that there is no difference in the means of auto and manual transmission), the probability of acquiring a dataset like the one we have where the means are 7 points different is only .1374% ... very very small. Hence we reject the null and say manual vehicles get better fuel economy than automatic vehicles. But ... should we stop there? We want to go another few steps and quantify this difference.
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
?plot
?legend
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.05, c("Predicted","Actual"), fill=c("black", "blue"))
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
library(Hmisc)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
p <- predict(step.model, interval="confidence")
PredictedMPG <- p[,1]
PredictedLower <- p[,2]
PredictedHigher <- p[,3]
FrankDF <- data.frame(Index = 1:32, Predicted = PredictedMPG, PredictedLower = PredictedLower, PredictedHigher = PredictedHigher)
png(filename="mtcarsModel.png", width=650, height=480)
step.model2 <- lm(mpg ~ factor(am):wt + factor(am):qsec, data = mtcars)
summary(step.model2)
library(Hmisc)
par(las=2, mar=c(8, 5, 4, 2))
plot(FrankDF$Index, mtcars$mpg, xaxt = "n", xlab = NA, ylim=c(0,35), main="MLR Predicted vs. Actual MPGs", ylab="MPGs", col="blue")
errbar(FrankDF$Index, FrankDF$Predicted, FrankDF$PredictedLower, FrankDF$PredictedHigher, add=T)
axis(1, at=1:32, labels=row.names(mtcars), cex.axis=0.8)
grid(nx=NA, ny=NULL)
legend("topleft", inset=0.01, c("Predicted","Actual"), fill=c("black", "blue"))
dev.off()
?shuttle
library(MASS)
?shuttle
shuttle[1,]
attach(shuttle)
LMshuttle <- lm(use ~ wind)
LMshuttle <- glm(use ~ wind, family = "binomial")
summary(LMshuttle)
plot(wind, LMshuttle$fitted, pch=19, col="blue")
LMshuttle <- glm(use ~ wind, family = "binomial")
shuttle[5,]
shuttle[1:5,]
shuttle[1:10,]
shuttle[1:15,]
summary(shuttle)
exp(coef(LMshuttle))
LMshuttle2 <- glm(use ~ wind * magn, family = "binomial")
exp(coef(LMshuttle2))
exp(coef(LMshuttle2))
LMshuttle3 <- glm(use~.,family="binomial")
LMshuttle3 <- glm(use~.,data=shuttle,family="binomial")
LMshuttle3 <- glm(use~wind,data=shuttle,family="binomial")
linearShuttle <- lm(use~wind, data=shuttle)
LMshuttle4 <- glm(use~wind-1,data=shuttle,family="binomial")
summary(LMshuttle3)
summary(LMshuttle4)
LMshuttle2 <- glm(use ~ wind * magn, family = "binomial")
exp(coef(LMshuttle2))
useyes<- shuttle$use=="auto"
useno<- !useyes
fityes <- glm(useyes ~ shuttle$wind,family="binomial")
fitno <- glm(useno ~ shuttle$wind,family="binomial")
exp(fityes$coeff)
exp(fitno$coeff)
fityes$coeff
fitno$coeff
InsectSprays
str(InsectSprays)
detach(shuttle)
attach(InsectSprays)
Spray <- glm(spray~count, family="poisson")
Spray <- glm(spray~count, family="pois")
Spray <- glm(spray~count, family=poisson())
boxplot(count ~ spray, data = InsectSprays,
xlab = "Type of spray", ylab = "Insect count",
main = "InsectSprays data", col = "lightgray")
boxplot(count ~ spray, data = InsectSprays,
xlab = "Type of spray", ylab = "Insect count",
main = "InsectSprays data", col = "lightgray")
glm1 = glm(count ~ spray, family=poisson())
summary(glm1)
exp(2.67415)
2.67415/0.05588
exp(coef(glm1))
1.0574713/0.1436782
coef(glm1)
LMshuttle <- glm(use ~ wind, family = "binomial")
exp(coef(LMshuttle))
LMshuttle2 <- glm(use ~ wind * magn, family = "binomial")
exp(coef(LMshuttle2))
LMshuttle2 <- glm(use ~ wind + magn, family = "binomial")
exp(coef(LMshuttle2))
attach(shuttle)
LMshuttle2 <- glm(use ~ wind + magn, family = "binomial")
exp(coef(LMshuttle2))
4/6
15*(4/6)
6/7
15*(6/7)
=12.85+10
12.85+10
15+15+22.85+40
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
CART <- train(Case ~ ., method = "rpart", data = training)
CART$finalMode
CART$finalModel
library(rattle)
fancyRpartPlot(CART$finalModel)
names(training)
summary(training$class)
head(training)
CART <- train(Class ~ ., method = "rpart", data = training)
CART$finalModel
fancyRpartPlot(CART$finalModel)
G <- gvisGeoChart(Exports, "Country", "Profit", options = list(width = (600), height = 400))
library(googlevis)
library(gvis)
library(gVis)
library(google)
library(googleVis)
G <- gvisGeoChart(Exports, "Country", "Profit", options = list(width = (600), height = 400))
plot(G)
plot(G)
print(G)
update.packages()
plot(model, log="y")
model$finalModel$importance
varImp(model)
library(caret)
varImp(model)
getwd()
setwd("Desktop/")
ls
ls()
setwd("../")
ls()
list.files()
setwd("pmlHumanActivityRecognition/")
list.files()
